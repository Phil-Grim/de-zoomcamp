{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa298d1",
   "metadata": {},
   "source": [
    " # Instancing Spark\n",
    " \n",
    " Importing the PySpark library and the SparkSession class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72505747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd55afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/USERNAME/spark/spark-3.4.2-bin-hadoop3/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f1cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6d80ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/23 09:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71c18a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"LocationID\",\"Borough\",\"Zone\",\"service_zone\"\r",
      "\r\n",
      "1,\"EWR\",\"Newark Airport\",\"EWR\"\r",
      "\r\n",
      "2,\"Queens\",\"Jamaica Bay\",\"Boro Zone\"\r",
      "\r\n",
      "3,\"Bronx\",\"Allerton/Pelham Gardens\",\"Boro Zone\"\r",
      "\r\n",
      "4,\"Manhattan\",\"Alphabet City\",\"Yellow Zone\"\r",
      "\r\n",
      "5,\"Staten Island\",\"Arden Heights\",\"Boro Zone\"\r",
      "\r\n",
      "6,\"Staten Island\",\"Arrochar/Fort Wadsworth\",\"Boro Zone\"\r",
      "\r\n",
      "7,\"Queens\",\"Astoria\",\"Boro Zone\"\r",
      "\r\n",
      "8,\"Queens\",\"Astoria Park\",\"Boro Zone\"\r",
      "\r\n",
      "9,\"Queens\",\"Auburndale\",\"Boro Zone\"\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# using a bash command to view this csv\n",
    "\n",
    "!head taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc15615",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f604529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-22 16:54:17--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240122T165418Z&X-Amz-Expires=300&X-Amz-Signature=aa17347fc636afb6a8a9a88fa8f7cd9b14f3c1b5984317c71c0379324758ec61&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-01-22 16:54:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/035746e8-4e24-47e8-a3ce-edcf6d1b11c7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240122%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240122T165418Z&X-Amz-Expires=300&X-Amz-Signature=aa17347fc636afb6a8a9a88fa8f7cd9b14f3c1b5984317c71c0379324758ec61&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-01.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 129967421 (124M) [application/octet-stream]\n",
      "Saving to: ‘fhvhv_tripdata_2021-01.csv.gz’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 123.95M  52.8MB/s    in 2.3s    \n",
      "\n",
      "2024-01-22 16:54:21 (52.8 MB/s) - ‘fhvhv_tripdata_2021-01.csv.gz’ saved [129967421/129967421]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading some high volume data - as spark is meant to deal with big data (this still isn't that big a file)\n",
    "\n",
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-01.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809464d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36dd996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7073b835",
   "metadata": {},
   "source": [
    "# Creating a Schema with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb547351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema # everything is string type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02fe2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 fhvhv_tripdata_2021-01.csv > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "659f0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66378330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee1fb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes # is now infering the types - didn't infer drop off and pick up as timestamps though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4571d767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2021-01-01 00:33:44</td>\n",
       "      <td>2021-01-01 00:49:07</td>\n",
       "      <td>230</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2021-01-01 00:55:19</td>\n",
       "      <td>2021-01-01 01:18:21</td>\n",
       "      <td>152</td>\n",
       "      <td>167</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:23:56</td>\n",
       "      <td>2021-01-01 00:38:05</td>\n",
       "      <td>233</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:42:51</td>\n",
       "      <td>2021-01-01 00:45:50</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:48:14</td>\n",
       "      <td>2021-01-01 01:08:42</td>\n",
       "      <td>143</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hvfhs_license_num dispatching_base_num      pickup_datetime  \\\n",
       "0            HV0003               B02682  2021-01-01 00:33:44   \n",
       "1            HV0003               B02682  2021-01-01 00:55:19   \n",
       "2            HV0003               B02764  2021-01-01 00:23:56   \n",
       "3            HV0003               B02764  2021-01-01 00:42:51   \n",
       "4            HV0003               B02764  2021-01-01 00:48:14   \n",
       "\n",
       "      dropoff_datetime  PULocationID  DOLocationID  SR_Flag  \n",
       "0  2021-01-01 00:49:07           230           166      NaN  \n",
       "1  2021-01-01 01:18:21           152           167      NaN  \n",
       "2  2021-01-01 00:38:05           233           142      NaN  \n",
       "3  2021-01-01 00:45:50           142           143      NaN  \n",
       "4  2021-01-01 01:08:42           143            78      NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0810628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hvfhs_license_num', StringType(), True), StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('SR_Flag', DoubleType(), True)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema # using spark session to create a spark dataframe\n",
    "# now that we've a pandas dataframe to create the spark dataframe, can see that not all the field types are strings anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bed67c",
   "metadata": {},
   "source": [
    "This datatype description is from Scala, which is the language that Spark is created with. Let's clean up the output and create our usable schema from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8698b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe88b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "        types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "        types.StructField('PULocationID', types.IntegerType(), True),\n",
    "        types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "        types.StructField('SR_Flag', types.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5978ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84bba2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   null|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   null|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   null|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   null|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   null|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   null|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   null|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27cbb77",
   "metadata": {},
   "source": [
    "# Partitions \n",
    "\n",
    "We will now create 24 partitions in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e555e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24) # this is a transformation, which is lazily executed (i.e. only executed when an action is run, like saving the df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cabff",
   "metadata": {},
   "source": [
    "We can now parquetize the dataframe. This will create 24 smaller parquet files.\n",
    "\n",
    "This operation may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845b6d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhvhv/2021/01/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5583909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 215M\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME    0 Jan 23 09:31 _SUCCESS\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00000-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00001-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00002-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00003-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00004-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00005-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00006-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00007-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00008-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00009-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00010-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00011-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00012-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00013-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00014-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00015-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00016-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00017-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00018-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00019-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00020-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00021-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00022-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 USERNAME USERNAME 9.0M Jan 23 09:31 part-00023-b5ebaa4e-8fb5-420f-a48b-e369c274360e-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhvhv/2021/01/ # 24 parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7bcff",
   "metadata": {},
   "source": [
    "# Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1aae18b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hvfhs_license_num: string, dispatching_base_num: string, pickup_datetime: timestamp, dropoff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe from the parquet files\n",
    "\n",
    "df = spark.read.parquet('fhvhv/2021/01/')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc628cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no need to specify schema as we did previously - parquet files (unlike csv files) contain the schema already\n",
    "\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d37b1f",
   "metadata": {},
   "source": [
    "# Functions and UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb4459",
   "metadata": {},
   "source": [
    "Besides the SQL and Pandas-like commands we've seen so far, Spark provides additional built-in functions that allow for more complex data manipulation. By convention, these functions are imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ff35734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff098525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2021-01-03 17:17:21|2021-01-03 17:26:18|         255|          34|   null|\n",
      "|           HV0003|              B02882|2021-01-05 22:14:07|2021-01-05 22:32:28|         189|         107|   null|\n",
      "|           HV0003|              B02867|2021-01-02 17:59:55|2021-01-02 18:10:39|          88|         137|   null|\n",
      "|           HV0003|              B02872|2021-01-02 23:57:54|2021-01-03 00:15:48|         238|         224|   null|\n",
      "|           HV0003|              B02875|2021-01-06 15:53:13|2021-01-06 16:07:07|         169|         208|   null|\n",
      "|           HV0003|              B02867|2021-01-07 07:35:24|2021-01-07 07:55:49|          75|          88|   null|\n",
      "|           HV0003|              B02764|2021-01-07 08:45:12|2021-01-07 08:51:17|         210|         210|   null|\n",
      "|           HV0003|              B02764|2021-01-02 15:44:26|2021-01-02 16:10:50|         243|          69|   null|\n",
      "|           HV0003|              B02869|2021-01-04 16:50:28|2021-01-04 16:57:43|         250|         213|   null|\n",
      "|           HV0003|              B02877|2021-01-03 10:30:34|2021-01-03 10:44:53|          87|          79|   null|\n",
      "|           HV0003|              B02617|2021-01-03 22:05:20|2021-01-03 22:27:55|          68|         181|   null|\n",
      "|           HV0003|              B02765|2021-01-04 08:01:02|2021-01-04 08:33:27|          95|         236|   null|\n",
      "|           HV0003|              B02835|2021-01-02 13:01:10|2021-01-02 13:08:11|         262|         236|   null|\n",
      "|           HV0005|              B02510|2021-01-04 05:25:51|2021-01-04 05:45:19|         225|         233|   null|\n",
      "|           HV0003|              B02836|2021-01-06 17:12:27|2021-01-06 17:46:56|         237|          83|   null|\n",
      "|           HV0005|              B02510|2021-01-05 07:07:33|2021-01-05 07:16:16|         231|          87|   null|\n",
      "|           HV0005|              B02510|2021-01-06 11:21:01|2021-01-06 11:31:58|          22|          26|   null|\n",
      "|           HV0003|              B02682|2021-01-04 09:05:18|2021-01-04 09:27:50|         159|          75|   null|\n",
      "|           HV0003|              B02869|2021-01-06 16:46:47|2021-01-06 17:50:24|         109|         119|   null|\n",
      "|           HV0003|              B02883|2021-01-06 08:03:47|2021-01-06 08:17:43|         145|         229|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0560c",
   "metadata": {},
   "source": [
    "Here's an example of built-in function usage:\n",
    "* `withColumn()` is a ***transformation*** that adds a new column to the dataframe.\n",
    "    * ***IMPORTANT***: adding a new column with the same name as a previously existing column will overwrite the existing column!\n",
    "* `select()` is another transformation that selects the stated columns.\n",
    "* `F.to_date()` is a built-in Spark function that converts a timestamp to date format (year, month and day only, no hour and minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d444f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-03|  2021-01-03|         255|          34|\n",
      "| 2021-01-05|  2021-01-05|         189|         107|\n",
      "| 2021-01-02|  2021-01-02|          88|         137|\n",
      "| 2021-01-02|  2021-01-03|         238|         224|\n",
      "| 2021-01-06|  2021-01-06|         169|         208|\n",
      "| 2021-01-07|  2021-01-07|          75|          88|\n",
      "| 2021-01-07|  2021-01-07|         210|         210|\n",
      "| 2021-01-02|  2021-01-02|         243|          69|\n",
      "| 2021-01-04|  2021-01-04|         250|         213|\n",
      "| 2021-01-03|  2021-01-03|          87|          79|\n",
      "| 2021-01-03|  2021-01-03|          68|         181|\n",
      "| 2021-01-04|  2021-01-04|          95|         236|\n",
      "| 2021-01-02|  2021-01-02|         262|         236|\n",
      "| 2021-01-04|  2021-01-04|         225|         233|\n",
      "| 2021-01-06|  2021-01-06|         237|          83|\n",
      "| 2021-01-05|  2021-01-05|         231|          87|\n",
      "| 2021-01-06|  2021-01-06|          22|          26|\n",
      "| 2021-01-04|  2021-01-04|         159|          75|\n",
      "| 2021-01-06|  2021-01-06|         109|         119|\n",
      "| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f02f85",
   "metadata": {},
   "source": [
    "A list of built-in functions is available [in the official Spark documentation page](https://spark.apache.org/docs/latest/api/sql/index.html).\n",
    "\n",
    "Besides these built-in functions, Spark allows us to create ***User Defined Functions*** (UDFs) with custom behavior for those instances where creating SQL queries for that behaviour becomes difficult both to manage and test.\n",
    "\n",
    "UDFs are regular functions which are then passed as parameters to a special builder. Let's create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a77acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crazy_stuff(base_num):\n",
    "    num = int(base_num[1:]) \n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}' # converting num to hex within the f string \n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'e/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2aa540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e/9ce'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crazy_stuff('B02510')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "beb36f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_stuff_udf = F.udf(crazy_stuff, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09ad8e",
   "metadata": {},
   "source": [
    "* `F.udf()` takes a function (`crazy_stuff()` in this example) as parameter as well as a return type for the function (a string in our example).\n",
    "* While `crazy_stuff()` is obviously non-sensical, UDFs are handy for things such as ML and other complex operations for which SQL isn't suitable or desirable. Python code is also easier to test than SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193eb89f",
   "metadata": {},
   "source": [
    "We can then use the UDF in transformations just as we would use built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d06ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+------------+------------+\n",
      "|base_id|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "|  e/9ce| 2021-01-03|  2021-01-03|         255|          34|\n",
      "|  e/b42| 2021-01-05|  2021-01-05|         189|         107|\n",
      "|  e/b33| 2021-01-02|  2021-01-02|          88|         137|\n",
      "|  e/b38| 2021-01-02|  2021-01-03|         238|         224|\n",
      "|  e/b3b| 2021-01-06|  2021-01-06|         169|         208|\n",
      "|  e/b33| 2021-01-07|  2021-01-07|          75|          88|\n",
      "|  e/acc| 2021-01-07|  2021-01-07|         210|         210|\n",
      "|  e/acc| 2021-01-02|  2021-01-02|         243|          69|\n",
      "|  e/b35| 2021-01-04|  2021-01-04|         250|         213|\n",
      "|  s/b3d| 2021-01-03|  2021-01-03|          87|          79|\n",
      "|  e/a39| 2021-01-03|  2021-01-03|          68|         181|\n",
      "|  s/acd| 2021-01-04|  2021-01-04|          95|         236|\n",
      "|  s/b13| 2021-01-02|  2021-01-02|         262|         236|\n",
      "|  e/9ce| 2021-01-04|  2021-01-04|         225|         233|\n",
      "|  e/b14| 2021-01-06|  2021-01-06|         237|          83|\n",
      "|  e/9ce| 2021-01-05|  2021-01-05|         231|          87|\n",
      "|  e/9ce| 2021-01-06|  2021-01-06|          22|          26|\n",
      "|  a/a7a| 2021-01-04|  2021-01-04|         159|          75|\n",
      "|  e/b35| 2021-01-06|  2021-01-06|         109|         119|\n",
      "|  a/b43| 2021-01-06|  2021-01-06|         145|         229|\n",
      "+-------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('base_id', crazy_stuff_udf(df.dispatching_base_num)) \\\n",
    "    .select('base_id', 'pickup_date', 'dropoff_date', 'PULocationID', 'DOLocationID') \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
